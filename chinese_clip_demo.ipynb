{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36166fed",
   "metadata": {},
   "source": [
    "!@pip install modelscope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2407f20",
   "metadata": {},
   "source": [
    "!pip install modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7424b48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_data.shape:torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.3281,  0.1886,  0.0079,  ...,  0.4142,  0.3857, -0.4796],\n",
       "          [-1.8078,  0.4325, -0.2976,  ..., -0.2563, -0.1678,  0.7117],\n",
       "          [-0.4261,  0.3471,  0.7903,  ..., -1.0470,  0.8813,  1.1001],\n",
       "          ...,\n",
       "          [ 0.0899,  0.2343,  0.1978,  ...,  0.1238, -1.1190,  0.7299],\n",
       "          [ 0.1859, -2.0315, -0.8164,  ...,  1.6072, -0.6566, -0.6520],\n",
       "          [-1.0012, -1.1342,  1.4717,  ...,  2.4817,  1.6954, -0.2797]],\n",
       "\n",
       "         [[ 0.1025, -0.9869,  1.4794,  ..., -0.3101,  0.9283, -0.5851],\n",
       "          [ 0.6504,  0.3255, -0.2406,  ...,  1.3563, -0.3143,  0.5066],\n",
       "          [ 0.9018, -0.9796,  2.0032,  ..., -0.9022,  0.5534, -0.0470],\n",
       "          ...,\n",
       "          [-1.2150, -1.2901, -0.5150,  ..., -0.0085, -0.0691, -1.7975],\n",
       "          [ 0.0955, -0.7213, -1.8479,  ..., -0.1106, -2.0152,  1.2165],\n",
       "          [ 1.0462, -0.1048,  2.1170,  ..., -0.0128,  0.7187,  0.9816]],\n",
       "\n",
       "         [[-1.5909,  0.3048,  0.3829,  ...,  0.1964, -0.3818,  0.0909],\n",
       "          [-0.2021, -1.2225, -0.9896,  ..., -1.5208,  1.0544, -2.1773],\n",
       "          [ 0.9172,  0.4842, -0.5989,  ..., -0.5275,  1.1146,  0.3962],\n",
       "          ...,\n",
       "          [-0.9101, -0.9647,  0.4176,  ...,  0.6441, -0.0439, -0.2070],\n",
       "          [ 1.1423, -0.4349,  1.4323,  ..., -1.2793,  0.2725,  0.0302],\n",
       "          [ 0.9065,  0.7007, -0.5722,  ...,  1.0193, -0.4239, -0.8059]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch;\n",
    "\n",
    "\n",
    "\n",
    "# 示例数据\n",
    "new_data = torch.randn(1, 3, 224, 224) # 示例数据\n",
    "\n",
    "\n",
    "\n",
    "print(f\"new_data.shape:{new_data.shape}\" );\n",
    "\n",
    "new_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6b7e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_transform() got an unexpected keyword argument 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_transform;\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 创建数据转换器\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m transform \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[0;32m      8\u001b[0m transform\n",
      "\u001b[1;31mTypeError\u001b[0m: create_transform() got an unexpected keyword argument 'transform'"
     ]
    }
   ],
   "source": [
    "from timm.data import create_transform;\n",
    "# 创建数据转换器\n",
    "\n",
    "\n",
    "transform = create_transform(input_size=new_data, transform=lambda x: x.float());\n",
    "\n",
    "\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c551b0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['ViT-B-16', 'ViT-L-14', 'ViT-L-14-336', 'ViT-H-14', 'RN50']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Ckpt download requires `modelscope`. Please install `modelscope` or download the ckpt manually and provide the local path so that we can continue.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Work\\AI\\stable_diffusion\\stable-diffusion-webui\\Chinese-CLIP\\cn_clip\\clip\\utils.py:67\u001b[0m, in \u001b[0;36m_download\u001b[1;34m(modelname, root, use_modelscope)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodelscope\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_download\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model_file_download\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'modelscope'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 如本地模型不存在，自动从ModelScope下载模型，需要提前安装`modelscope`包\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m model, preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mViT-B-16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_modelscope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     13\u001b[0m image \u001b[38;5;241m=\u001b[39m preprocess(Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexamples/pokemon.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mD:\\Work\\AI\\stable_diffusion\\stable-diffusion-webui\\Chinese-CLIP\\cn_clip\\clip\\utils.py:109\u001b[0m, in \u001b[0;36mload_from_name\u001b[1;34m(name, device, download_root, vision_model_name, text_model_name, input_resolution, use_modelscope)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_from_name\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m, device: Union[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    107\u001b[0m                    download_root: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, vision_model_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, text_model_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, input_resolution: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, use_modelscope: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _MODELS:\n\u001b[1;32m--> 109\u001b[0m         model_path \u001b[38;5;241m=\u001b[39m \u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpanduser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m~/.cache/clip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_modelscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m         model_name, model_input_resolution \u001b[38;5;241m=\u001b[39m _MODEL_INFO[name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstruct\u001b[39m\u001b[38;5;124m'\u001b[39m], _MODEL_INFO[name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_resolution\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(name):\n",
      "File \u001b[1;32mD:\\Work\\AI\\stable_diffusion\\stable-diffusion-webui\\Chinese-CLIP\\cn_clip\\clip\\utils.py:69\u001b[0m, in \u001b[0;36m_download\u001b[1;34m(modelname, root, use_modelscope)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodelscope\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_download\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model_file_download\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _:\n\u001b[1;32m---> 69\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCkpt download requires `modelscope`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install `modelscope` or download the ckpt manually and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovide the local path so that we can continue.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m         )\n\u001b[0;32m     75\u001b[0m     local_path \u001b[38;5;241m=\u001b[39m model_file_download(\n\u001b[0;32m     76\u001b[0m         model_id\u001b[38;5;241m=\u001b[39m_MODELSCOPE_ORG \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m reponame,\n\u001b[0;32m     77\u001b[0m         file_path\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m     78\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mroot,\n\u001b[0;32m     79\u001b[0m     )\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Ckpt download requires `modelscope`. Please install `modelscope` or download the ckpt manually and provide the local path so that we can continue."
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from PIL import Image\n",
    "\n",
    "import cn_clip.clip as clip\n",
    "from cn_clip.clip import load_from_name, available_models\n",
    "print(\"Available models:\", available_models())  \n",
    "# Available models: ['ViT-B-16', 'ViT-L-14', 'ViT-L-14-336', 'ViT-H-14', 'RN50']\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# 如本地模型不存在，自动从ModelScope下载模型，需要提前安装`modelscope`包\n",
    "model, preprocess = load_from_name(\"ViT-B-16\", device=device, download_root='./', use_modelscope=True)\n",
    "model.eval()\n",
    "image = preprocess(Image.open(\"examples/pokemon.jpeg\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"杰尼龟\", \"妙蛙种子\", \"小火龙\", \"皮卡丘\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    # 对特征进行归一化，请使用归一化后的图文特征用于下游任务\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True) \n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)    \n",
    "\n",
    "    logits_per_image, logits_per_text = model.get_similarity(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # [[1.268734e-03 5.436878e-02 6.795761e-04 9.436829e-01]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf5336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chinese-Clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
