# Chinese-CLIP 项目功能详细说明

## 📋 项目概述

**Chinese-CLIP** 是 CLIP 模型的中文版本，使用大规模中文图文对数据（~2亿）训练，旨在帮助用户快速实现中文领域的图文理解、检索和分类任务。

### 核心特点
- 🎯 **中文优化**：专门针对中文数据优化，在中文图文任务上表现优异
- 🚀 **多模型支持**：提供5种不同规模的预训练模型（RN50、ViT-B-16、ViT-L-14、ViT-L-14-336、ViT-H-14）
- 🔧 **完整工具链**：包含训练、评估、部署的完整解决方案
- 📦 **易于使用**：提供简洁的API接口，几行代码即可使用

---

## 🎯 主要功能模块

### 1. **图文特征提取与相似度计算** ⭐

**功能描述**：提取图像和文本的特征向量，并计算它们之间的相似度。

**使用场景**：
- 图像-文本匹配
- 图文相似度计算
- 多模态特征提取

**代码示例**（`test_demo.py`）：
```python
import torch 
from PIL import Image
import cn_clip.clip as clip
from cn_clip.clip import load_from_name

# 加载模型
model, preprocess = load_from_name("ViT-B-16", device="cuda")
model.eval()

# 处理图像和文本
image = preprocess(Image.open("examples/pokemon.jpeg")).unsqueeze(0).to(device)
text = clip.tokenize(["杰尼龟", "妙蛙种子", "小火龙", "皮卡丘"]).to(device)

# 提取特征并计算相似度
with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text)
    # 归一化特征
    image_features /= image_features.norm(dim=-1, keepdim=True) 
    text_features /= text_features.norm(dim=-1, keepdim=True)
    # 计算相似度
    logits_per_image, logits_per_text = model.get_similarity(image, text)
    probs = logits_per_image.softmax(dim=-1).cpu().numpy()
```

**支持的功能**：
- ✅ 图像特征编码：`model.encode_image()`
- ✅ 文本特征编码：`model.encode_text()`
- ✅ 相似度计算：`model.get_similarity()`
- ✅ 自动模型下载（支持 ModelScope）

---

### 2. **跨模态检索** 🔍

**功能描述**：实现文本到图像（Text-to-Image）和图像到文本（Image-to-Text）的双向检索。

**使用场景**：
- 以图搜图
- 以文搜图
- 图像标注
- 内容推荐系统

**支持的数据集**：
- **MUGE Retrieval**：大规模中文图文检索数据集
- **Flickr30K-CN**：中文版本的Flickr30K数据集
- **COCO-CN**：中文版本的COCO数据集

**功能模块**（`cn_clip/eval/`）：
- `evaluation.py`：标准评估脚本
- `extract_features.py`：特征提取工具
- `make_topk_predictions.py`：Top-K预测生成

**评估指标**：
- R@1, R@5, R@10（Recall@K）
- MR（Mean Rank）

**性能表现**（MUGE数据集）：
- Zero-shot：R@1=63.0%, R@5=84.1%, R@10=89.2%
- Finetune：R@1=68.9%, R@5=88.7%, R@10=93.1%

---

### 3. **模型训练** 🏋️

**功能描述**：支持从零开始训练或基于预训练模型进行微调（Finetune）。

**训练模式**：
- **预训练（Pretraining）**：在大规模图文对上训练
- **微调（Finetuning）**：在特定数据集上微调

**核心模块**（`cn_clip/training/`）：
- `main.py`：训练主入口
- `train.py`：训练逻辑
- `data.py`：数据处理
- `params.py`：参数配置
- `scheduler.py`：学习率调度
- `logger.py`：日志记录

**训练特性**：
- ✅ **分布式训练**：支持多GPU、多节点训练
- ✅ **梯度累积**：模拟更大batch size
- ✅ **混合精度训练**：支持FP16/FP32/AMP
- ✅ **梯度检查点**：节省显存（`--grad-checkpointing`）
- ✅ **FlashAttention**：加速训练，降低显存（`--use-flash-attention`）
- ✅ **FLIP策略**：提升训练效果（`--use-flip`）
- ✅ **数据增强**：支持图像增强（`--use-augment`）

**训练脚本**（`run_scripts/`）：
- `muge_finetune_vit-b-16_rbt-base.sh`：MUGE数据集微调
- `flickr30k_finetune_vit-b-16_rbt-base.sh`：Flickr30K-CN微调
- `coco-cn_finetune_vit-b-16_rbt-base.sh`：COCO-CN微调

**当前训练配置**（`run_training.bat`）：
- 模型：ViT-B-16 + RoBERTa-wwm-ext-base-chinese
- 数据集：MUGE（训练集和验证集）
- Batch Size：48
- 学习率：3e-06
- 最大轮数：1 epoch
- 优化器：AdamW（weight decay=0.001）

---

### 4. **零样本图像分类** 🎨

**功能描述**：无需训练即可对图像进行分类，支持多种分类任务。

**支持的数据集**（ELEVATER Benchmark）：
- CIFAR10, CIFAR100
- DTD（纹理分类）
- EuroSAT（卫星图像）
- FER（面部表情识别）
- FGVC（细粒度分类）
- KITTI（自动驾驶场景）
- MNIST（手写数字）
- PC（宠物分类）
- VOC（物体检测）

**功能模块**（`cn_clip/eval/`）：
- `zeroshot_evaluation.py`：零样本评估主脚本
- `imagenet_zeroshot_templates.py`：ImageNet模板
- `cvinw_zeroshot_templates.py`：CVINW模板

**性能表现**：
- CIFAR10：96.0%
- CIFAR100：79.7%
- VOC：84.9%

---

### 5. **模型部署** 🚀

**功能描述**：将PyTorch模型转换为部署格式，提升推理速度。

**支持的部署格式**：
- **ONNX**：跨平台部署
- **TensorRT**：NVIDIA GPU加速
- **CoreML**：Apple设备部署

**部署模块**（`cn_clip/deploy/`）：
- `pytorch_to_onnx.py`：PyTorch → ONNX转换
- `onnx_to_tensorrt.py`：ONNX → TensorRT转换
- `pytorch_to_coreml.py`：PyTorch → CoreML转换
- `speed_benchmark.py`：速度基准测试
- `tensorrt_utils.py`：TensorRT工具函数

**部署优势**：
- ⚡ 推理速度提升（TensorRT可提升2-5倍）
- 💾 显存占用降低
- 📱 支持移动端部署（CoreML）

---

### 6. **数据处理** 📊

**功能描述**：数据预处理和格式转换工具。

**预处理模块**（`cn_clip/preprocess/`）：
- `build_lmdb_dataset.py`：构建LMDB格式数据集
  - 将图像和文本对转换为高效的LMDB格式
  - 支持大规模数据集处理
  - 当前项目已处理MUGE数据集为LMDB格式

**数据格式**：
- **图像**：支持常见图像格式（JPEG、PNG等）
- **文本**：支持中文文本，使用RoBERTa tokenizer
- **数据集格式**：
  - TSV格式：`train_imgs.tsv`, `valid_imgs.tsv`
  - JSONL格式：`train_texts.jsonl`, `valid_texts.jsonl`
  - LMDB格式：用于高效训练

**当前数据集**（`datapath/datasets/MUGE/`）：
- 训练集：`lmdb/train/`
- 验证集：`lmdb/valid/`
- 测试集：`lmdb/test/`

---

### 7. **知识蒸馏** 🎓

**功能描述**：使用大模型指导小模型训练，提升小模型性能。

**功能特性**：
- 支持基于ModelScope的蒸馏
- 教师-学生模型架构
- 提升小模型在中文任务上的表现

**相关文档**：`distillation.md`

---

## 📦 模型规格

### 可用模型列表

| 模型名称 | 参数量 | 视觉骨架 | 文本骨架 | 分辨率 | 下载源 |
|---------|--------|---------|---------|--------|--------|
| RN50 | 77M | ResNet50 | RBT3 | 224 | HuggingFace / ModelScope |
| ViT-B-16 | 188M | ViT-B/16 | RoBERTa-wwm-Base | 224 | HuggingFace / ModelScope |
| ViT-L-14 | 406M | ViT-L/14 | RoBERTa-wwm-Base | 224 | HuggingFace / ModelScope |
| ViT-L-14-336 | 407M | ViT-L/14 | RoBERTa-wwm-Base | 336 | HuggingFace / ModelScope |
| ViT-H-14 | 958M | ViT-H/14 | RoBERTa-wwm-Large | 224 | HuggingFace / ModelScope |

### 模型配置
所有模型配置位于：`cn_clip/clip/model_configs/`

---

## 🛠️ 项目结构

```
Chinese-CLIP/
├── cn_clip/                    # 核心代码包
│   ├── clip/                   # CLIP模型实现
│   │   ├── model.py           # 模型定义
│   │   ├── modeling_bert.py   # BERT文本编码器
│   │   ├── bert_tokenizer.py  # 中文分词器
│   │   └── model_configs/     # 模型配置文件
│   ├── training/              # 训练模块
│   │   ├── main.py            # 训练入口
│   │   ├── train.py           # 训练逻辑
│   │   ├── data.py            # 数据处理
│   │   └── params.py          # 参数配置
│   ├── eval/                  # 评估模块
│   │   ├── evaluation.py      # 检索评估
│   │   ├── zeroshot_evaluation.py  # 零样本分类
│   │   └── extract_features.py     # 特征提取
│   ├── deploy/                # 部署模块
│   │   ├── pytorch_to_onnx.py
│   │   ├── onnx_to_tensorrt.py
│   │   └── pytorch_to_coreml.py
│   └── preprocess/            # 数据预处理
│       └── build_lmdb_dataset.py
├── datapath/                   # 数据目录
│   ├── datasets/              # 数据集
│   │   └── MUGE/              # MUGE数据集
│   ├── pretrained_weights/    # 预训练权重
│   └── experiments/           # 实验输出
├── run_scripts/               # 训练脚本
├── examples/                  # 示例文件
├── test_demo.py              # API使用示例
├── run_training.bat          # Windows训练脚本
└── README.md                 # 项目文档
```

---

## 🚀 快速开始

### 1. 安装依赖
```bash
pip install -r requirements.txt
pip install -e .  # 安装cn_clip包
```

### 2. 使用API（最简单）
```python
python test_demo.py
```

### 3. 运行训练
```bash
# Windows
run_training.bat

# Linux/Mac
bash run_scripts/muge_finetune_vit-b-16_rbt-base.sh
```

### 4. 评估模型
```bash
python cn_clip/eval/evaluation.py [参数]
```

---

## 📈 性能指标

### 图文检索性能（MUGE数据集）

| 模型 | Zero-shot R@1 | Finetune R@1 |
|------|---------------|--------------|
| Wukong | 42.7% | 52.7% |
| R2D2 | 49.5% | 60.1% |
| **CN-CLIP** | **63.0%** | **68.9%** |

### 零样本分类性能

| 数据集 | CN-CLIP准确率 |
|--------|--------------|
| CIFAR10 | 96.0% |
| CIFAR100 | 79.7% |
| VOC | 84.9% |

---

## 🔧 技术特性

### 训练优化技术
- ✅ **分布式训练**：支持多GPU、多节点
- ✅ **混合精度**：FP16/FP32/AMP
- ✅ **梯度累积**：模拟大batch size
- ✅ **梯度检查点**：节省显存
- ✅ **FlashAttention**：加速注意力计算
- ✅ **FLIP策略**：提升训练效果
- ✅ **数据增强**：图像增强技术

### 模型特性
- ✅ **多模态对齐**：图像-文本联合学习
- ✅ **中文优化**：专门针对中文数据
- ✅ **零样本能力**：无需微调即可使用
- ✅ **可扩展性**：支持多种模型规模

---

## 📝 使用场景

1. **图像搜索**：根据文本描述搜索相关图像
2. **图像标注**：自动为图像生成文本描述
3. **内容推荐**：基于图文相似度的推荐系统
4. **零样本分类**：无需训练即可分类新类别
5. **多模态理解**：理解图像和文本的语义关系
6. **产品部署**：转换为ONNX/TensorRT进行生产部署

---

## 📚 相关资源

- **论文**：[Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335)
- **模型下载**：[HuggingFace](https://huggingface.co/OFA-Sys) / [ModelScope](https://www.modelscope.cn)
- **在线Demo**：[ModelScope Studio](https://www.modelscope.cn/studios/damo/chinese_clip_applications/summary)
- **技术博客**：[Qwen Blog](https://qwenlm.github.io/zh/blog/chinese-clip/)

---

## ⚠️ 注意事项

1. **Windows环境**：需要设置 `USE_LIBUV=0` 环境变量
2. **CUDA要求**：需要CUDA >= 10.2
3. **显存需求**：根据模型规模，需要4GB-24GB显存
4. **数据格式**：训练需要使用LMDB格式数据集

---

## 📞 支持

如有问题，请参考：
- `README.md`：完整使用文档
- `启动训练说明.md`：训练启动指南
- `deployment.md`：部署文档
- `distillation.md`：知识蒸馏文档

---

**最后更新**：2024年12月

